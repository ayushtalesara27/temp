# -*- coding: utf-8 -*-
"""Current Best Testing strategy - CTREND score v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10lalxF5WHj1EpdhQstIjnWT6BtG-50Ln
"""

!pip3 install nsepy
!pip3 install ta
!pip3 install mplfinance
!pip3 install pandas_ta
!pip3 install smartapi
!pip3 install smartapi-python
!pip3 install websocket-client
!pip3 install pyotp
!pip3 install selenium
!pip3 install logzero
!pip3 install websocket-client
!pip3 install pmdarima

from google.colab import drive
drive.mount('/content/gdrive')

windows_base_path = r"C:\Users\ayush\Documents"
linux_base_path = "/home/ayush/Documents"
collab_base_path = r"/content/gdrive/MyDrive/Stocks/trader"

# Commented out IPython magic to ensure Python compatibility.
#Import libraries
!cp {collab_base_path}/my_imports.py .
from my_imports import *

#Run magic commands
# %matplotlib inline
# Reset all warning filters
warnings.resetwarnings()
# Suppress all FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning, module="pandas")
# Suppress repeated display of other warnings
warnings.filterwarnings(action='once')

!pip install numpy_financial

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy_financial as nf

#############################################
# HELPER: Get Daily Close Price for a Stock on a Given Date
#############################################
def get_daily_close(symbol, date):
    """
    Returns the daily close price for the given symbol at the given date.
    If the exact date is not available, returns the last available price before the date.
    """
    if symbol not in df_data_storage:
        return None
    df = df_data_storage[symbol]
    if date in df.index:
        return df.loc[date, 'Close']
    else:
        #print("AYUSH", date)
        available = df.loc[:date]
        if not available.empty:
            return available.iloc[-1]['Close']
        else:
            return None

from collections import defaultdict

def compute_money_weighted_irr(executed_trades_df):
    """
    Computes the money-weighted return (XIRR) based on trade-level cash flows:
      - Negative flow on each trade entry
      - Positive flow on each trade exit
    Idle capital is effectively 'out of the market'.

    Returns:
      Annualized IRR (float). E.g., 0.10 for 10% per year.
    """
    # Dictionary keyed by date -> cumulative cashflow
    cashflows_by_date = defaultdict(float)

    for _, trade in executed_trades_df.iterrows():
        entry_dt = trade['entry_date']
        exit_dt  = trade['exit_date']
        if pd.isnull(entry_dt) or pd.isnull(exit_dt):
            continue  # skip any malformed rows

        # Outflow on entry
        cost = trade['entry_price'] * trade['shares']
        cashflows_by_date[entry_dt] -= cost

        # Inflow on exit
        proceeds = trade['exit_price'] * trade['shares']
        cashflows_by_date[exit_dt] += proceeds

    # Convert dict -> list of (date, flow), sorted by date
    dated_flows = sorted(cashflows_by_date.items(), key=lambda x: x[0])

    if not dated_flows:
        return np.nan  # no trades => no IRR

    # Separate into amounts and dates for xirr
    flows = [cf[1] for cf in dated_flows]
    dates = [cf[0] for cf in dated_flows]

    # Compute XIRR
    try:
        irr_value = xirr(dates, flows)  # annualized money-weighted return
    except Exception as e:
        irr_value = np.nan  # sometimes xirr can fail if flows are pathological

    return irr_value


#############################################
# HELPER: Compute Sharpe Ratio from an Equity Series
#############################################
def compute_sharpe_ratio(equity_series, annual_risk_free_rate=0, trading_days=252):
    """
    Computes the annualized Sharpe ratio from a pandas Series of portfolio equity values.
    Assumes risk_free_rate is 0 if not provided.
    """
    # Compute daily returns
    daily_returns = equity_series.pct_change().dropna()
    if daily_returns.std() == 0:
        return np.nan
    risk_free_rate = (1 + annual_risk_free_rate)**(1/trading_days) - 1
    # Compute daily Sharpe ratio and annualize it
    sharpe_daily = (daily_returns.mean() - risk_free_rate) / daily_returns.std()
    sharpe_annualized = sharpe_daily * np.sqrt(trading_days)
    return sharpe_annualized

import os
import pandas as pd
import numpy as np
from collections import deque, defaultdict

# Strategy parameters
folder_path_zigzags = os.path.join(collab_base_path, "ZIGZAGS_BLOOMBERG_FILTERED")
pkl_name_1x            = "bloomberg_bse500_1x_new.pkl"
pkl_name_3x            = "bloomberg_bse500_3x_new.pkl"
index_sym           = "BSE500"
ENTRY_THRESH        = 7.0      # enter when score crosses UP through this
EXIT_THRESH         = 6.0      # exit when score drops below this
SHARES              = 1        # not used by backtest, kept for reference
csv_path_1x = os.path.join(folder_path_zigzags, pkl_name_1x)
csv_path_3x = os.path.join(folder_path_zigzags, pkl_name_3x)
df_data_storage_1x = load_df_data_storage(csv_path_1x)
df_data_storage_3x = load_df_data_storage(csv_path_3x)
# df_data_storage = {**df_data_storage_1x, **df_data_storage_3x}

# ───────────────────────────────────────────────────────────────────
#  Merge 3× scores into 1× dict – align starts, reindex, forward-fill
# ───────────────────────────────────────────────────────────────────
for sym in list(df_data_storage_1x.keys()):
    if sym not in df_data_storage_3x:
        df_data_storage_1x.pop(sym)
        print(f"[WARN] {sym}: absent from 3× data – removed from 1× dict.")
        continue

    df1x = df_data_storage_1x[sym]
    df3x = df_data_storage_3x[sym]

    # 1) Align the start date to the first record present in 3×
    first_date_3x = df3x.index.min()
    df1x = df1x.loc[df1x.index >= first_date_3x].copy()

    trimmed_rows = len(df_data_storage_1x[sym]) - len(df1x)
    if trimmed_rows:
        print(f"{sym}: cut {trimmed_rows} rows (dates < {first_date_3x.date()}).")

    # 2) Re-index 3× score onto the new 1× index and forward-fill gaps
    score_3x_ff = (
        df3x["final_score"]
        .reindex(df1x.index)  # align to 1× dates
        .ffill()              # carry last value forward
        .rename("final_score_3x")
    )

    # 3) Insert as new column
    df1x["final_score_3x"] = score_3x_ff

    df1x = df1x.rename(columns={"final_score": "final_score_1x"})

    # 4) Store back
    df_data_storage_1x[sym] = df1x

df_data_storage = df_data_storage_1x.copy()

df_data_storage.pop(index_sym)

df_data_storage.keys()

import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.FileHandler("indicator_errors.log")
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

import numpy as np
import pandas as pd
import logging

from ta.momentum     import (
    rsi,
    stoch,
    stoch_signal,
    stochrsi_k,
    stochrsi_d,
    pvo,
    pvo_signal,
    pvo_hist
)
from ta.trend        import (
    sma_indicator,
    macd,
    macd_signal,
    macd_diff,
    cci,
    ADXIndicator
)
from ta.volatility   import (
    bollinger_hband,
    bollinger_lband,
    bollinger_pband,
    bollinger_wband,
    average_true_range
)
from ta.volume       import (
    on_balance_volume,
    chaikin_money_flow,
    money_flow_index
)

logger = logging.getLogger(__name__)

def enrich_indicators(df):
    """Add standard TA features to df safely (never errors out)."""
    def safe_call(func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error("Indicator %s failed: %s", func.__name__, e)
            return pd.Series(np.nan, index=df.index)

    # ── Momentum oscillators ────────────────────────────────────────────────
    df['RSI']        = safe_call(rsi,        df['Close'], window=14, fillna=False)
    df['STOCH_K']    = safe_call(stoch,      df['High'], df['Low'], df['Close'], window=14, smooth_window=3, fillna=False)
    df['STOCH_D']    = safe_call(stoch_signal, df['High'], df['Low'], df['Close'], window=14, smooth_window=3, fillna=False)
    df['STOCHRSI_K'] = safe_call(stochrsi_k, df['Close'], window=14, smooth1=3, smooth2=3, fillna=False)
    df['STOCHRSI_D'] = safe_call(stochrsi_d, df['Close'], window=14, smooth1=3, smooth2=3, fillna=False)
    df['CCI']        = safe_call(cci,        df['High'], df['Low'], df['Close'], window=20, constant=0.015, fillna=False)

    # ── Moving averages ─────────────────────────────────────────────────────
    for w in (3, 5, 10, 20, 50, 100, 200):
        df[f'SMA_{w}'] = safe_call(sma_indicator, df['Close'], window=w, fillna=False)

    df['MACD']        = safe_call(macd,        df['Close'], window_fast=12, window_slow=26, fillna=False)
    df['MACD_SIGNAL'] = safe_call(macd_signal, df['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)
    df['MACD_DIFF']   = safe_call(macd_diff,   df['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)

    # ── Volatility / Bollinger bands ────────────────────────────────────────
    df['BB_H'] = safe_call(bollinger_hband, df['Close'], window=20, fillna=False) / df['Close'] - 1
    df['BB_L'] = safe_call(bollinger_lband, df['Close'], window=20, fillna=False) / df['Close'] - 1
    df['BB_P'] = safe_call(bollinger_pband, df['Close'], window=20, fillna=False)
    df['BB_W'] = safe_call(bollinger_wband, df['Close'], window=20, fillna=False)

    # ── Volume & money-flow ────────────────────────────────────────────────
    df['OBV'] = safe_call(on_balance_volume, df['Close'], df['Volume'], fillna=False)
    df['CMF'] = safe_call(chaikin_money_flow, df['High'], df['Low'], df['Close'], df['Volume'], window=20, fillna=False)
    df['MFI'] = safe_call(money_flow_index, df['High'], df['Low'], df['Close'], df['Volume'], window=14, fillna=False)

    # ── ADX & directional indicators ───────────────────────────────────────
    try:
        adx = ADXIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=14, fillna=False)
        df['ADX']     = adx.adx()
        df['ADX_POS'] = adx.adx_pos()
        df['ADX_NEG'] = adx.adx_neg()
    except Exception as e:
        logger.error("ADXIndicator failed: %s", e)
        df['ADX'] = np.nan
        df['ADX_POS'] = np.nan
        df['ADX_NEG'] = np.nan

    # ── Average True Range ─────────────────────────────────────────────────
    df['ATR'] = safe_call(average_true_range, df['High'], df['Low'], df['Close'], window=14, fillna=False)

    # ── Volume SMAs (manual rolling mean) ──────────────────────────────────
    for w in (3, 5, 10, 20, 50, 100, 200):
        df[f'volsma_{w}'] = df['Volume'].rolling(window=w, min_periods=1).mean() / df['Volume']

    # ── Percentage Volume Oscillator (PVO) ────────────────────────────────
    df['volmacd']             = safe_call(pvo,        df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)
    df['volmacd_signal']      = safe_call(pvo_signal, df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)
    df['volmacd_diff_signal'] = safe_call(pvo_hist,   df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)

    return df

# Apply across all symbols before your weekly-resample step
df_data_storage = {
    sym: enrich_indicators(df.copy())
    for sym, df in df_data_storage.items()
}

# Sanity check: print the columns of the first symbol
print(df_data_storage[next(iter(df_data_storage))].columns.tolist())

df_data_storage['3M IN Equity'].columns


# 1) Identify all the indicator columns in one of your DataFrames:
sample = df_data_storage[next(iter(df_data_storage))]
# drop price/volume/index columns
indicator_cols = [
    c for c in sample.columns
    if c not in ['Open','High','Low','Close','Volume','in_index']
]

# 2) Build a weekly‐frequency table for every symbol,
#    carrying forward indicators, in_index, and fwd_ret:
weekly_features = {}
for sym, df in df_data_storage.items():
    # a) Resample indicators to Friday closes
    w = df[indicator_cols].resample('W-FRI').last()

    # **Add the actual Close price for each Friday**
    w['Close'] = df['Close'].resample('W-FRI').last()
    # b) in_index and forward return
    w['in_index'] = df['in_index'].resample('W-FRI').last().ffill()
    w['fwd_ret']  = df['Close'].resample('W-FRI').last().pct_change().shift(-1)
    weekly_features[sym] = w

# 3) Concatenate into a MultiIndex DataFrame
panel = pd.concat(weekly_features, names=['symbol','date'])

# 4) Rank‐scale each indicator cross‐sectionally to [−0.5, +0.5]
for col in indicator_cols:
    # rank(pct=True) gives [0,1], subtract .5 → [−0.5,+0.5]
    panel[f'{col}_rs'] = (
        panel.groupby(level='date')[col]
             .rank(pct=True, method='first')
             .sub(0.5)
    )

# 5) Check the new columns
print(panel.head())
print("\nRank-scaled cols:", [c for c in panel.columns if c.endswith('_rs')])

rank_cols = [c for c in panel.columns if c.endswith('_rs')]   # ±0.5 inputs


!pip install --no-cache-dir cupy-cuda12x

!nvcc --version
!find /usr -maxdepth 3 -type f -name 'libnvrtc.so*'

import cupy as cp
print(hex(cp.cuda.runtime.runtimeGetVersion()))
print(cp.cuda.runtime.runtimeGetVersion())

from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import TimeSeriesSplit


WIN      = 52                     # one-year look-back
WINDOW = WIN
# alpha_g  = np.logspace(-5, 2, 60) # α-grid, cf. paper
alpha_g = np.logspace(-7,  3, 100)   # cover [1e-7 … 1e3] with finer steps
L1RATIO  = 0.5
DEBUG = True


def dbg(msg):
    if DEBUG: print(msg)

# ─────────────────────────────────────────────────────────────────────────────
# 1.  STAGE-1  UNIVARIATE FM  →  WEEKLY FORECAST MATRIX  R_hat
# ─────────────────────────────────────────────────────────────────────────────
dates = panel.index.get_level_values('date').unique().sort_values()

R_hat_parts = []     # we’ll build one big DF at the end

for pos in range(WINDOW, len(dates)):
    end, start = dates[pos], dates[pos-WINDOW]

    mask = (panel.index.get_level_values('date') >= start) & \
           (panel.index.get_level_values('date') <  end)
    s = panel.loc[mask]


    y = s['fwd_ret'].values
    ok_y = ~np.isnan(y)

    # container for this Friday’s forecasts (shape: N_symbols × J_factors)
    preds_this_week = {}

    for col in rank_cols:                       # incl. your extra factors
        x = s[col].values
        ok = ok_y & ~np.isnan(x)
        if ok.sum() < 20:
            continue                            # thin data guard

        # ----- OLS with constant:  r = α + β z  ---------------------------
        Xreg   = np.c_[np.ones(ok.sum()), x[ok]]       # add intercept
        alpha, beta = np.linalg.lstsq(Xreg, y[ok], rcond=None)[0]

        # forecast for *today* only
        idx_today = panel.loc[pd.IndexSlice[:, end], :].index
        x_today   = panel.loc[idx_today, col].values
        preds_this_week[col] = alpha + beta * x_today

    # combine into DataFrame for the current Friday
    if preds_this_week:
        idx_today = panel.loc[pd.IndexSlice[:, end], :].index
        R_hat_parts.append(
            pd.DataFrame(preds_this_week, index=idx_today)
        )

    if DEBUG and pos % 25 == 0:
        dbg(f"[{end.date()}] stage-1 finished ({pos+1-WINDOW}/{len(dates)-WINDOW})")

# concat all weeks; drop cols never forecast
R_hat = pd.concat(R_hat_parts).sort_index()
if DEBUG:
    dbg(f"R_hat overall shape: {R_hat.shape}")

enet2 = ElasticNetCV(
    alphas     = alpha_g,
    l1_ratio   = L1RATIO,
    positive   = False,
    cv         = TimeSeriesSplit(n_splits=5),   # keeps chronology
    max_iter   = 1_000_000,
    n_jobs     = -1,
)

# make sure *every* forecast is present – neutral-fill with zero
# R_hat_filled = R_hat.fillna(0.0)
R_hat_filled = (R_hat
                .groupby(level='date')
                .transform(lambda x: x.fillna(x.mean()))
                .fillna(0.0))
dbg(R_hat_filled.memory_usage(deep=True).sum() / 1e6)



# ─────────────────────────────────────────────────────────────────────────────
# Collect full weekly ElasticNet coefficients
coef_df = pd.DataFrame(columns=R_hat_filled.columns, dtype=float)
# ─────────────────────────────────────────────────────────────────────────────


if DEBUG:
    # average number of active (non-zero) forecasts per symbol-week
    nz_counts = (R_hat_filled.astype(bool)).sum(axis=1)
    avg_nz    = nz_counts.groupby(level='date').mean().mean()
    dbg(f"Avg active forecasts per row: {avg_nz:.1f} / {len(rank_cols)}")

# storage containers
theta_hist = {}                         # {Friday : pd.Series}
panel['CTREND'] = np.nan                # init

for end in dates[WIN:]:                 # loop over Fridays – starting after 1-yr burn-in
    start = end - pd.Timedelta(weeks=WIN)

    in_win = (R_hat_filled.index.get_level_values('date') >= start) & \
             (R_hat_filled.index.get_level_values('date') <  end)

    X_win = R_hat_filled.loc[in_win].astype('float32').values
    y_win = panel.loc[R_hat_filled.index[in_win], 'fwd_ret'].values

    ok = ~np.isnan(y_win)

    # require at least 20 observations per factor (paper’s rule)
    min_rows = 20 * len(rank_cols)
    if ok.sum() < min_rows:
        continue

    if DEBUG:
        # idx_train is the MultiIndex of the rows we’re actually fitting on
        idx_all   = R_hat_filled.index[in_win]   # all rows in the window
        idx_train = idx_all[ok]                 # then mask by ok
        n_rows    = len(idx_train)
        n_symbols = idx_train.get_level_values('symbol').nunique()
        dbg(f"[{end.date()}] training on {n_rows:,} rows ({n_symbols} symbols)")

        # ── DEBUG: report design matrix size ─────────────────────────────────
    if DEBUG:
        X_design = X_win[ok]
        dbg(f"[{end.date()}] Stage-2 design: {X_design.shape[0]:,} rows × {X_design.shape[1]:,} factors")

    enet2.fit(X_win[ok], y_win[ok])

    # record the full coefficient vector for this Friday
    coef_df.loc[end] = enet2.coef_

        # ── DEBUG: model performance & θ stats ───────────────────────────────
    if DEBUG:
        train_r2 = enet2.score(X_design, y_win[ok])
        dbg(f"[{end.date()}] Train R² = {train_r2:.4f}")
        theta    = pd.Series(enet2.coef_, index=R_hat_filled.columns)
        dbg(f"[{end.date()}] θ count={ (theta>0).sum() }, "
            f"mean={theta[theta>0].mean():.4f}, std={theta[theta>0].std():.4f}")

    # θ̂  – keep only positive weights as in eq. (11) :contentReference[oaicite:1]{index=1}
    theta = pd.Series(enet2.coef_, index=R_hat_filled.columns)
    theta = theta[theta > 0]

    theta_hist[end] = theta                # record for inspection / back-test

    # ----------------  build this Friday’s CTREND  ----------------
    idx_today = panel.loc[pd.IndexSlice[:, end], :].index      # rows for this Friday

    if not theta.empty:
        panel.loc[idx_today, 'CTREND'] = (
            R_hat_filled.loc[idx_today, theta.index]           # align rows+cols
                        .mean(axis=1)
        )
    else:
        panel.loc[idx_today, 'CTREND'] = np.nan

    if DEBUG:
        dbg(f"[{end.date()}] α*={enet2.alpha_:8.4g}  kept θ={len(theta):2d}")

# final tidy-up
panel['CTREND'] = panel['CTREND'].fillna(0.0)      # neutral where no signal
# recompute 'ctrend' Series from the panel we just filled
ctrend = panel['CTREND'].dropna()

if DEBUG:
    wstats = ctrend.groupby(level='date').mean().describe()
    dbg("\nCTREND weekly cross-sectional mean stats:")
    dbg(wstats.to_string())

# θ sparsity over time
pd.DataFrame({d: s.size for d,s in theta_hist.items()}, index=['#θ>0']).T.plot()

# CTREND coverage
coverage = panel['CTREND'].groupby(level='date').apply(lambda x: x.notna().mean())
coverage.plot(title="Fraction of symbols with CTREND each week")
# ─────────────────────────────────────────────────────────────────────────────
# 4.  QUICK LONG–SHORT SANITY CHECK  (equal-weight quintiles)
#      – you can replace with your full back-test block later
# ─────────────────────────────────────────────────────────────────────────────
def q5_q1_week(df):
    q = pd.qcut(df['CTREND'], 5, labels=False, duplicates='drop')
    long  = df[q == 4]['fwd_ret'].mean()
    short = df[q == 0]['fwd_ret'].mean()
    return long - short

ls_week = (panel.dropna(subset=['CTREND', 'fwd_ret'])
                 .groupby(level='date')
                 .apply(q5_q1_week))

if DEBUG:
    dbg("\nLong-short first 5 weeks (%):")
    dbg((ls_week.head()*100).round(2).to_string())
    sharpe = ls_week.mean() / ls_week.std(ddof=0) * np.sqrt(52)
    dbg(f"\nEqual-weight QS Sharpe ≈ {sharpe:.2f}")

et = panel['CTREND']
print(et.describe())
et.hist(bins=50)

import plotly.express as px

def plot_indicator_importance(
    coef_df,
    absolute: bool = True,
    top_n: int | None = None
) -> None:
    """
    Visualize normalized indicator importance (summing to 100%).

    Parameters
    ----------
    coef_df : pd.DataFrame
        Rows = dates, columns = factor names, values = ElasticNet weights.
    absolute : bool, default True
        If True, use mean(abs(coef)); else use signed mean(coef).
    top_n : int or None
        If set, only show the top_n indicators by normalized importance.
    """
    # 1) compute the “raw” importances
    if absolute:
        raw_imp = coef_df.abs().mean()
        title  = "Average Absolute Importance (normalized to 100%)"
    else:
        raw_imp = coef_df.mean().abs()
        title  = "Average Signed Importance (abs, normalized to 100%)"

    # 2) normalize so they sum to 100%
    norm_imp = raw_imp / raw_imp.sum() * 100

    # 3) sort (descending) and optionally trim to top_n
    norm_imp = norm_imp.sort_values(ascending=False)
    if top_n is not None:
        norm_imp = norm_imp.head(top_n)

    # 4) plot
    fig = px.bar(
        x=norm_imp.index,
        y=norm_imp.values,
        labels={"x": "Indicator", "y": "Importance (%)"},
        title=title
    )
    fig.update_layout(
        xaxis_tickangle=-45,
        margin=dict(l=40, r=40, t=60, b=120)
    )
    fig.show()

plot_indicator_importance(coef_df, absolute=True, top_n=60)

import numpy as np
import pandas as pd
from pandas.api.types import CategoricalDtype

# ─────────────────────────────────────────────────────────────────────────────
# 0.  CONFIG
# ─────────────────────────────────────────────────────────────────────────────
# Add a mode flag: True to clip returns at ± limits, False for raw returns
CLIP_RETURNS = True
UPPER_CLIP = 0.10
LOWER_CLIP = -0.10
cost_bp = 0
perf    = []
print("DT         | total |  non-null  |   top95    |   bot05    | long | short | status")

# ─────────────────────────────────────────────────────────────────────────────
# 1.  LOOP OVER WEEKS
# ─────────────────────────────────────────────────────────────────────────────
for dt, grp in panel.groupby(level='date'):
    total  = len(grp)
    # drop any symbols missing CTREND or fwd_ret
    grp    = grp.dropna(subset=['CTREND','fwd_ret'])
    # only trade symbols that are in the index
    grp    = grp[grp['in_index'] == 1]
    idx_ok = len(grp)

    if grp.empty:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} |     —     |     —     |    0 |     0 | SKIP")
        continue

    top95     = grp['CTREND'].quantile(0.95)
    bot05     = grp['CTREND'].quantile(0.05)
    long_grp  = grp[grp['CTREND'] > top95].copy()
    short_grp = grp[grp['CTREND'] < bot05].copy()

    n_long  = len(long_grp)
    n_short = len(short_grp)
    if n_long == 0 or n_short == 0:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | SKIP")
        continue

    #── compute realized vol over the *prior* 4 weeks ──────────────────────
    vol_long = (
        long_grp['fwd_ret']
             .shift(1)
             .rolling(4, min_periods=4)
             .std()
             .ffill()
    )
    vol_short = (
        short_grp['fwd_ret']
               .shift(1)
               .rolling(4, min_periods=4)
               .std()
               .ffill()
    )

    vol_long  = vol_long.clip(lower=1e-3)
    vol_short = vol_short.clip(lower=1e-3)


    # vol_long = 1
    # vol_short = 1

    # ── build weights by scaling CTREND by volatility ─────────────────────
    w_long  = long_grp['CTREND']  / vol_long
    w_short = -short_grp['CTREND'] / vol_short

    # collapse the MultiIndex so that w_long.index == symbols only
    w_long.index  = w_long.index.get_level_values('symbol')
    w_short.index = w_short.index.get_level_values('symbol')

    # ── record per‐symbol allocations & entry info ─────────────────────
    # fraction of leg notional each symbol gets
    norm_w_long = w_long / w_long.sum()
    alloc_long_pct  = {
        sym: pct for sym, pct in norm_w_long.items()
        if not np.isnan(pct)
    }

    norm_w_short = w_short / w_short.sum()
    alloc_short_pct = {
        sym: pct for sym, pct in norm_w_short.items()
        if not np.isnan(pct)
    }

    # entry prices (Friday close) and individual returns with clipping
    # entry_long_price  = {sym: weekly_data[sym].loc[dt]
    #                      for sym in long_grp.index.get_level_values('symbol')}
    # entry_short_price = {sym: weekly_data[sym].loc[dt]
    #                      for sym in short_grp.index.get_level_values('symbol')}

    entry_long_price = (
    long_grp['Close']
      .reset_index(level='date', drop=True)
      .to_dict()
    )
    entry_short_price = (
        short_grp['Close']
        .reset_index(level='date', drop=True)
        .to_dict()
    )

    # ── SKIP if either leg has zero total weight ─────────────────────────
    if w_long.sum() == 0 or w_short.sum() == 0:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | ---- zero weight leg, SKIP")
        continue

    # ── compute weighted average returns ──────────────────────────────────
    # clip returns to avoid outliers
    # lr = long_grp['fwd_ret'].clip(lower=-0.10)
    # sr = short_grp['fwd_ret'].clip(upper=+0.10)
    if CLIP_RETURNS:
        lr = long_grp['fwd_ret'].clip(lower=LOWER_CLIP)
        sr = short_grp['fwd_ret'].clip(upper=UPPER_CLIP)
    else:
        lr = long_grp['fwd_ret']
        sr = short_grp['fwd_ret']

    # drop the 'date' level from the multi‐index → keys become just symbols
    ret_long_indiv  = lr.reset_index(level='date', drop=True).to_dict()
    ret_short_indiv = sr.reset_index(level='date', drop=True).to_dict()


    # compute exit price by applying the (clipped) return to the entry price
    exit_long_price = {
        sym: entry_long_price[sym] * (1 + ret_long_indiv[sym])
        for sym in entry_long_price
    }
    exit_short_price = {
        sym: entry_short_price[sym] * (1 + ret_short_indiv[sym])
        for sym in entry_short_price
    }

    long_ret  = (lr * w_long).sum()  / w_long.sum()
    short_ret = (sr * w_short).sum() / w_short.sum()
    spread    = long_ret - short_ret - 2*(cost_bp/1e4)

    # ── record symbol lists for post-mortem ──────────────────────────────
    long_syms  = long_grp.index.get_level_values('symbol').tolist()
    short_syms = short_grp.index.get_level_values('symbol').tolist()

    perf.append({
        'date':       dt,
        'total':      total,
        'non_null':   idx_ok,
        'top95':      top95,
        'bot05':      bot05,
        'n_long':     n_long,
        'n_short':    n_short,
        'long_ret':   long_ret,
        'short_ret':  short_ret,
        'spread':     spread,
        'long_syms':         long_syms,
        'short_syms':        short_syms,
        # new per‐trade details
        'long_alloc_pct':    alloc_long_pct,
        'long_entry_price':  entry_long_price,
        'long_ret_indiv':     ret_long_indiv,
        'long_exit_price':   exit_long_price,
        'short_alloc_pct':   alloc_short_pct,
        'short_entry_price': entry_short_price,
        'short_ret_indiv':    ret_short_indiv,
        'short_exit_price':  exit_short_price
    })

    print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | APPEND")

# ─────────────────────────────────────────────────────────────────────────────
# 2.  BUILD SUMMARY DATAFRAME & METRICS
# ─────────────────────────────────────────────────────────────────────────────
perf_df = pd.DataFrame(perf).set_index('date')

print("\nRESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
print("TOTAL WEEKS:", len(perf_df))
print(f"Avg weekly Q5–Q1 : {perf_df['spread'].mean():.2%}")
print(f"Ann. Sharpe       : {perf_df['spread'].mean()/perf_df['spread'].std(ddof=0)*np.sqrt(52):.2f}")
print(perf_df.tail())