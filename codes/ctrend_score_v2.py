# -*- coding: utf-8 -*-
"""Current Best Testing strategy - CTREND score v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10lalxF5WHj1EpdhQstIjnWT6BtG-50Ln
"""

!pip3 install nsepy
!pip3 install ta
!pip3 install mplfinance
!pip3 install pandas_ta
!pip3 install smartapi
!pip3 install smartapi-python
!pip3 install websocket-client
!pip3 install pyotp
!pip3 install selenium
!pip3 install logzero
!pip3 install websocket-client
!pip3 install pmdarima

from google.colab import drive
drive.mount('/content/gdrive')

windows_base_path = r"C:\Users\ayush\Documents"
linux_base_path = "/home/ayush/Documents"
collab_base_path = r"/content/gdrive/MyDrive/Stocks/trader"

# Commented out IPython magic to ensure Python compatibility.
#Import libraries
!cp {collab_base_path}/my_imports.py .
from my_imports import *

#Run magic commands
# %matplotlib inline
# Reset all warning filters
warnings.resetwarnings()
# Suppress all FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning, module="pandas")
# Suppress repeated display of other warnings
warnings.filterwarnings(action='once')

!pip install numpy_financial

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy_financial as nf

#############################################
# HELPER: Get Daily Close Price for a Stock on a Given Date
#############################################
def get_daily_close(symbol, date):
    """
    Returns the daily close price for the given symbol at the given date.
    If the exact date is not available, returns the last available price before the date.
    """
    if symbol not in df_data_storage:
        return None
    df = df_data_storage[symbol]
    if date in df.index:
        return df.loc[date, 'Close']
    else:
        #print("AYUSH", date)
        available = df.loc[:date]
        if not available.empty:
            return available.iloc[-1]['Close']
        else:
            return None

from collections import defaultdict

def compute_money_weighted_irr(executed_trades_df):
    """
    Computes the money-weighted return (XIRR) based on trade-level cash flows:
      - Negative flow on each trade entry
      - Positive flow on each trade exit
    Idle capital is effectively 'out of the market'.

    Returns:
      Annualized IRR (float). E.g., 0.10 for 10% per year.
    """
    # Dictionary keyed by date -> cumulative cashflow
    cashflows_by_date = defaultdict(float)

    for _, trade in executed_trades_df.iterrows():
        entry_dt = trade['entry_date']
        exit_dt  = trade['exit_date']
        if pd.isnull(entry_dt) or pd.isnull(exit_dt):
            continue  # skip any malformed rows

        # Outflow on entry
        cost = trade['entry_price'] * trade['shares']
        cashflows_by_date[entry_dt] -= cost

        # Inflow on exit
        proceeds = trade['exit_price'] * trade['shares']
        cashflows_by_date[exit_dt] += proceeds

    # Convert dict -> list of (date, flow), sorted by date
    dated_flows = sorted(cashflows_by_date.items(), key=lambda x: x[0])

    if not dated_flows:
        return np.nan  # no trades => no IRR

    # Separate into amounts and dates for xirr
    flows = [cf[1] for cf in dated_flows]
    dates = [cf[0] for cf in dated_flows]

    # Compute XIRR
    try:
        irr_value = xirr(dates, flows)  # annualized money-weighted return
    except Exception as e:
        irr_value = np.nan  # sometimes xirr can fail if flows are pathological

    return irr_value


#############################################
# HELPER: Compute Sharpe Ratio from an Equity Series
#############################################
def compute_sharpe_ratio(equity_series, annual_risk_free_rate=0, trading_days=252):
    """
    Computes the annualized Sharpe ratio from a pandas Series of portfolio equity values.
    Assumes risk_free_rate is 0 if not provided.
    """
    # Compute daily returns
    daily_returns = equity_series.pct_change().dropna()
    if daily_returns.std() == 0:
        return np.nan
    risk_free_rate = (1 + annual_risk_free_rate)**(1/trading_days) - 1
    # Compute daily Sharpe ratio and annualize it
    sharpe_daily = (daily_returns.mean() - risk_free_rate) / daily_returns.std()
    sharpe_annualized = sharpe_daily * np.sqrt(trading_days)
    return sharpe_annualized

import os
import pandas as pd
import numpy as np
from collections import deque, defaultdict

# Strategy parameters
folder_path_zigzags = os.path.join(collab_base_path, "ZIGZAGS_BLOOMBERG_FILTERED")
pkl_name_1x            = "bloomberg_bse500_1x_new.pkl"
pkl_name_3x            = "bloomberg_bse500_3x_new.pkl"
index_sym           = "BSE500"
ENTRY_THRESH        = 7.0      # enter when score crosses UP through this
EXIT_THRESH         = 6.0      # exit when score drops below this
SHARES              = 1        # not used by backtest, kept for reference
csv_path_1x = os.path.join(folder_path_zigzags, pkl_name_1x)
csv_path_3x = os.path.join(folder_path_zigzags, pkl_name_3x)
df_data_storage_1x = load_df_data_storage(csv_path_1x)
df_data_storage_3x = load_df_data_storage(csv_path_3x)
# df_data_storage = {**df_data_storage_1x, **df_data_storage_3x}

# ───────────────────────────────────────────────────────────────────
#  Merge 3× scores into 1× dict – align starts, reindex, forward-fill
# ───────────────────────────────────────────────────────────────────
for sym in list(df_data_storage_1x.keys()):
    if sym not in df_data_storage_3x:
        df_data_storage_1x.pop(sym)
        print(f"[WARN] {sym}: absent from 3× data – removed from 1× dict.")
        continue

    df1x = df_data_storage_1x[sym]
    df3x = df_data_storage_3x[sym]

    # 1) Align the start date to the first record present in 3×
    first_date_3x = df3x.index.min()
    df1x = df1x.loc[df1x.index >= first_date_3x].copy()

    trimmed_rows = len(df_data_storage_1x[sym]) - len(df1x)
    if trimmed_rows:
        print(f"{sym}: cut {trimmed_rows} rows (dates < {first_date_3x.date()}).")

    # 2) Re-index 3× score onto the new 1× index and forward-fill gaps
    score_3x_ff = (
        df3x["final_score"]
        .reindex(df1x.index)  # align to 1× dates
        .ffill()              # carry last value forward
        .rename("final_score_3x")
    )

    # 3) Insert as new column
    df1x["final_score_3x"] = score_3x_ff

    df1x = df1x.rename(columns={"final_score": "final_score_1x"})

    # 4) Store back
    df_data_storage_1x[sym] = df1x

df_data_storage = df_data_storage_1x.copy()

df_data_storage.pop(index_sym)

df_data_storage.keys()

import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.FileHandler("indicator_errors.log")
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

import numpy as np
import pandas as pd
import logging

from ta.momentum     import (
    rsi,
    stoch,
    stoch_signal,
    stochrsi_k,
    stochrsi_d,
    pvo,
    pvo_signal,
    pvo_hist
)
from ta.trend        import (
    sma_indicator,
    macd,
    macd_signal,
    macd_diff,
    cci,
    ADXIndicator
)
from ta.volatility   import (
    bollinger_hband,
    bollinger_lband,
    bollinger_pband,
    bollinger_wband,
    average_true_range
)
from ta.volume       import (
    on_balance_volume,
    chaikin_money_flow,
    money_flow_index
)

logger = logging.getLogger(__name__)

def enrich_indicators(df):
    """Add standard TA features to df safely (never errors out)."""
    def safe_call(func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error("Indicator %s failed: %s", func.__name__, e)
            return pd.Series(np.nan, index=df.index)

    # ── Momentum oscillators ────────────────────────────────────────────────
    df['RSI']        = safe_call(rsi,        df['Close'], window=14, fillna=False)
    df['STOCH_K']    = safe_call(stoch,      df['High'], df['Low'], df['Close'], window=14, smooth_window=3, fillna=False)
    df['STOCH_D']    = safe_call(stoch_signal, df['High'], df['Low'], df['Close'], window=14, smooth_window=3, fillna=False)
    df['STOCHRSI_K'] = safe_call(stochrsi_k, df['Close'], window=14, smooth1=3, smooth2=3, fillna=False)
    df['STOCHRSI_D'] = safe_call(stochrsi_d, df['Close'], window=14, smooth1=3, smooth2=3, fillna=False)
    df['CCI']        = safe_call(cci,        df['High'], df['Low'], df['Close'], window=20, constant=0.015, fillna=False)

    # ── Moving averages ─────────────────────────────────────────────────────
    for w in (3, 5, 10, 20, 50, 100, 200):
        df[f'SMA_{w}'] = safe_call(sma_indicator, df['Close'], window=w, fillna=False)

    df['MACD']        = safe_call(macd,        df['Close'], window_fast=12, window_slow=26, fillna=False)
    df['MACD_SIGNAL'] = safe_call(macd_signal, df['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)
    df['MACD_DIFF']   = safe_call(macd_diff,   df['Close'], window_fast=12, window_slow=26, window_sign=9, fillna=False)

    # ── Volatility / Bollinger bands ────────────────────────────────────────
    df['BB_H'] = safe_call(bollinger_hband, df['Close'], window=20, fillna=False) / df['Close'] - 1
    df['BB_L'] = safe_call(bollinger_lband, df['Close'], window=20, fillna=False) / df['Close'] - 1
    df['BB_P'] = safe_call(bollinger_pband, df['Close'], window=20, fillna=False)
    df['BB_W'] = safe_call(bollinger_wband, df['Close'], window=20, fillna=False)

    # ── Volume & money-flow ────────────────────────────────────────────────
    df['OBV'] = safe_call(on_balance_volume, df['Close'], df['Volume'], fillna=False)
    df['CMF'] = safe_call(chaikin_money_flow, df['High'], df['Low'], df['Close'], df['Volume'], window=20, fillna=False)
    df['MFI'] = safe_call(money_flow_index, df['High'], df['Low'], df['Close'], df['Volume'], window=14, fillna=False)

    # ── ADX & directional indicators ───────────────────────────────────────
    try:
        adx = ADXIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=14, fillna=False)
        df['ADX']     = adx.adx()
        df['ADX_POS'] = adx.adx_pos()
        df['ADX_NEG'] = adx.adx_neg()
    except Exception as e:
        logger.error("ADXIndicator failed: %s", e)
        df['ADX'] = np.nan
        df['ADX_POS'] = np.nan
        df['ADX_NEG'] = np.nan

    # ── Average True Range ─────────────────────────────────────────────────
    df['ATR'] = safe_call(average_true_range, df['High'], df['Low'], df['Close'], window=14, fillna=False)

    # ── Volume SMAs (manual rolling mean) ──────────────────────────────────
    for w in (3, 5, 10, 20, 50, 100, 200):
        df[f'volsma_{w}'] = df['Volume'].rolling(window=w, min_periods=1).mean() / df['Volume']

    # ── Percentage Volume Oscillator (PVO) ────────────────────────────────
    df['volmacd']             = safe_call(pvo,        df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)
    df['volmacd_signal']      = safe_call(pvo_signal, df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)
    df['volmacd_diff_signal'] = safe_call(pvo_hist,   df['Volume'], window_slow=26, window_fast=12, window_sign=9, fillna=False)

    return df

# Apply across all symbols before your weekly-resample step
df_data_storage = {
    sym: enrich_indicators(df.copy())
    for sym, df in df_data_storage.items()
}

# Sanity check: print the columns of the first symbol
print(df_data_storage[next(iter(df_data_storage))].columns.tolist())

df_data_storage['3M IN Equity'].columns

# import pandas as pd

# # 1) Build a weekly‐frequency table per symbol
# weekly_data = {}
# for sym, df in df_data_storage.items():
#     # a) Resample to Friday closes
#     w = df['Close'].resample('W-FRI').last().to_frame('close')

#     # b) Carry forward the in_index flag (we care about membership on formation date)
#     w['in_index'] = df['in_index'].resample('W-FRI').last()

#     # c) Compute 1-week ahead total‐return (forward shift)
#     w['fwd_ret'] = w['close'].pct_change(periods=1).shift(-1)

#     weekly_data[sym] = w

# # 2) Combine into a single DataFrame with a MultiIndex (symbol, date)
# panel = (
#     pd.concat(weekly_data, names=['symbol', 'date'])
#       .reset_index()
#       .set_index(['symbol', 'date'])
# )

# # 3) Quick sanity check
# print(panel.head())
# print("\nColumns:", panel.columns.tolist())

import pandas as pd

# 1) Identify all the indicator columns in one of your DataFrames:
sample = df_data_storage[next(iter(df_data_storage))]
# drop price/volume/index columns
indicator_cols = [
    c for c in sample.columns
    if c not in ['Open','High','Low','Close','Volume','in_index']
]

# 2) Build a weekly‐frequency table for every symbol,
#    carrying forward indicators, in_index, and fwd_ret:
weekly_features = {}
for sym, df in df_data_storage.items():
    # a) Resample indicators to Friday closes
    w = df[indicator_cols].resample('W-FRI').last()

    # **Add the actual Close price for each Friday**
    w['Close'] = df['Close'].resample('W-FRI').last()
    # b) in_index and forward return
    w['in_index'] = df['in_index'].resample('W-FRI').last().ffill()
    w['fwd_ret']  = df['Close'].resample('W-FRI').last().pct_change().shift(-1)
    weekly_features[sym] = w

# 3) Concatenate into a MultiIndex DataFrame
panel = pd.concat(weekly_features, names=['symbol','date'])

# 4) Rank‐scale each indicator cross‐sectionally to [−0.5, +0.5]
for col in indicator_cols:
    # rank(pct=True) gives [0,1], subtract .5 → [−0.5,+0.5]
    panel[f'{col}_rs'] = (
        panel.groupby(level='date')[col]
             .rank(pct=True, method='first')
             .sub(0.5)
    )

# 5) Check the new columns
print(panel.head())
print("\nRank-scaled cols:", [c for c in panel.columns if c.endswith('_rs')])

rank_cols = [c for c in panel.columns if c.endswith('_rs')]   # ±0.5 inputs

# # ─────────────────────────────────────────────────────────────────────────────
# # 1.  IDENTIFY PREDICTORS & TARGET
# # ─────────────────────────────────────────────────────────────────────────────

# TARGET    = 'fwd_ret'                                        # 1-wk ahead TR

# # ─────────────────────────────────────────────────────────────────────────────
# # 2.  FAST CROSS-SECTIONAL OLS (no intercept, cf. Fama–MacBeth 1973)
# # ─────────────────────────────────────────────────────────────────────────────
# import numpy as np, pandas as pd
# from collections import defaultdict

# def cross_section_beta(X: np.ndarray, y: np.ndarray) -> np.ndarray:
#     """β = (X'X)^-1 X'y via NumPy lstsq — ~2-4× faster than statsmodels."""
#     beta, *_ = np.linalg.lstsq(X, y, rcond=None)
#     return beta

# window = 52                                           # 52 Fridays ≈ 1 yr
# dates  = panel.index.get_level_values('date').unique().sort_values()

# raw_betas = defaultdict(list)                         # {factor: [(dt,β),…]}

# for pos in range(window, len(dates)):
#     end   = dates[pos]
#     start = dates[pos - window]

#     mask  = (panel.index.get_level_values('date') > start) & \
#             (panel.index.get_level_values('date') < end)
#     s     = panel.loc[mask]

#     X     = s[rank_cols].to_numpy(float)
#     y     = s[TARGET].to_numpy(float)

#     valid = ~np.isnan(X).any(1) & ~np.isnan(y)        # drop rows w/ NaN
#     if valid.sum() < len(rank_cols) + 5:              # guard thin weeks
#         continue

#     β = cross_section_beta(X[valid], y[valid])

#     for c, b in zip(rank_cols, β):
#         raw_betas[c].append((end, b))

# beta_df = pd.DataFrame({c: pd.Series(dict(v)) for c, v in raw_betas.items()}
#                       ).sort_index()

# # ─────────────────────────────────────────────────────────────────────────────
# # 3.  EWMA SMOOTHING  (λ = 0.95  ⇒  half-life ≈ 20 weeks)
# # ---------------------------------------------------------------------------—
# lambda_      = 0.975
# beta_smooth  = beta_df.ewm(alpha=1 - lambda_, adjust=False).mean()

# # Align universe so every Friday in beta_smooth is in panel
# panel_weekly = panel.loc[pd.IndexSlice[:, beta_smooth.index], :]
# panel_weekly['in_index'] = (
#     panel_weekly['in_index']
#                 .groupby(level='symbol')
#                 .ffill()
# )

!pip install --no-cache-dir cupy-cuda12x

!nvcc --version
!find /usr -maxdepth 3 -type f -name 'libnvrtc.so*'

import cupy as cp
print(hex(cp.cuda.runtime.runtimeGetVersion()))
print(cp.cuda.runtime.runtimeGetVersion())

# # ─────────────────────────────────────────────────────────────────────────────
# # CONFIG & DEBUG
# # ─────────────────────────────────────────────────────────────────────────────
# DEBUG   = True         # flip to False for silent run
# WINDOW  = 52           # 52 Fridays ≈ 1 year
# alpha_grid = np.logspace(-5, 2, 60)
# L1RATIO = 0.5


# dbg(f"\nR_hat built  –  shape {R_hat.shape} (rows × factors)")

# # 1) Replace NaN forecasts with 0  (neutral signal)
# R_hat_filled = R_hat.fillna(0.0)


# enet2 = ElasticNetCV(
#     alphas      = alpha_grid,
#     l1_ratio    = L1RATIO,
#     positive    = False,
#     cv          = TimeSeriesSplit(5),
#     max_iter    = 200_000,
#     n_jobs      = -1
# )


# # ---------------------------------------------------------------------------
# # 2.  STAGE-2  ELASTIC-NET COMBINER  θ̂  (CS-C-ENet)
# # ---------------------------------------------------------------------------
# from sklearn.linear_model import ElasticNetCV
# from sklearn.model_selection import TimeSeriesSplit

# theta_hist = {}

# for end in dates[WINDOW:]:
#     idx = (R_hat_filled.index.get_level_values('date') >= start) & \
#       (R_hat_filled.index.get_level_values('date') <  end)

#     X_raw = R_hat_filled.loc[idx].astype('float32')
#     y_raw = panel.loc[R_hat_filled.index[idx], 'fwd_ret']

#     ok = ~y_raw.isna()
#     if ok.sum() < 500:          # thin guard
#         continue

#     X = X_raw.values[ok.values]
#     y = y_raw.values[ok.values]

#     enet2.fit(X[ok], y[ok])
#     θ = pd.Series(enet2.coef_, index=R_hat_filled.columns)
#     θ = θ[θ > 0]
#     theta_hist[end] = θ

#     # CTREND for this Friday: mean of selected forecasts
#     today_idx = R_hat_filled.index.get_level_values('date') == end
#     panel.loc[today_idx, 'CTREND'] = R_hat_filled.loc[today_idx, θ.index].mean(1)


# # recompute 'ctrend' Series from the panel we just filled
# ctrend = panel['CTREND'].dropna()

# if DEBUG:
#     wstats = ctrend.groupby(level='date').mean().describe()
#     dbg("\nCTREND weekly cross-sectional mean stats:")
#     dbg(wstats.to_string())

# # ─────────────────────────────────────────────────────────────────────────────
# # 4.  QUICK LONG–SHORT SANITY CHECK  (equal-weight quintiles)
# #      – you can replace with your full back-test block later
# # ─────────────────────────────────────────────────────────────────────────────
# def q5_q1_week(df):
#     q = pd.qcut(df['CTREND'], 5, labels=False, duplicates='drop')
#     long  = df[q == 4]['fwd_ret'].mean()
#     short = df[q == 0]['fwd_ret'].mean()
#     return long - short

# ls_week = (panel.dropna(subset=['CTREND', 'fwd_ret'])
#                  .groupby(level='date')
#                  .apply(q5_q1_week))

# if DEBUG:
#     dbg("\nLong-short first 5 weeks (%):")
#     dbg((ls_week.head()*100).round(2).to_string())
#     sharpe = ls_week.mean() / ls_week.std(ddof=0) * np.sqrt(52)
#     dbg(f"\nEqual-weight QS Sharpe ≈ {sharpe:.2f}")

from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import TimeSeriesSplit


WIN      = 52                     # one-year look-back
WINDOW = WIN
# alpha_g  = np.logspace(-5, 2, 60) # α-grid, cf. paper
alpha_g = np.logspace(-7,  3, 100)   # cover [1e-7 … 1e3] with finer steps
L1RATIO  = 0.5
DEBUG = True


def dbg(msg):
    if DEBUG: print(msg)

# ─────────────────────────────────────────────────────────────────────────────
# 1.  STAGE-1  UNIVARIATE FM  →  WEEKLY FORECAST MATRIX  R_hat
# ─────────────────────────────────────────────────────────────────────────────
dates = panel.index.get_level_values('date').unique().sort_values()

R_hat_parts = []     # we’ll build one big DF at the end

for pos in range(WINDOW, len(dates)):
    end, start = dates[pos], dates[pos-WINDOW]

    mask = (panel.index.get_level_values('date') >= start) & \
           (panel.index.get_level_values('date') <  end)
    s = panel.loc[mask]


    y = s['fwd_ret'].values
    ok_y = ~np.isnan(y)

    # container for this Friday’s forecasts (shape: N_symbols × J_factors)
    preds_this_week = {}

    for col in rank_cols:                       # incl. your extra factors
        x = s[col].values
        ok = ok_y & ~np.isnan(x)
        if ok.sum() < 20:
            continue                            # thin data guard

        # ----- OLS with constant:  r = α + β z  ---------------------------
        Xreg   = np.c_[np.ones(ok.sum()), x[ok]]       # add intercept
        alpha, beta = np.linalg.lstsq(Xreg, y[ok], rcond=None)[0]

        # forecast for *today* only
        idx_today = panel.loc[pd.IndexSlice[:, end], :].index
        x_today   = panel.loc[idx_today, col].values
        preds_this_week[col] = alpha + beta * x_today

    # combine into DataFrame for the current Friday
    if preds_this_week:
        idx_today = panel.loc[pd.IndexSlice[:, end], :].index
        R_hat_parts.append(
            pd.DataFrame(preds_this_week, index=idx_today)
        )

    if DEBUG and pos % 25 == 0:
        dbg(f"[{end.date()}] stage-1 finished ({pos+1-WINDOW}/{len(dates)-WINDOW})")

# concat all weeks; drop cols never forecast
R_hat = pd.concat(R_hat_parts).sort_index()
if DEBUG:
    dbg(f"R_hat overall shape: {R_hat.shape}")

enet2 = ElasticNetCV(
    alphas     = alpha_g,
    l1_ratio   = L1RATIO,
    positive   = False,
    cv         = TimeSeriesSplit(n_splits=5),   # keeps chronology
    max_iter   = 1_000_000,
    n_jobs     = -1,
)

# make sure *every* forecast is present – neutral-fill with zero
# R_hat_filled = R_hat.fillna(0.0)
R_hat_filled = (R_hat
                .groupby(level='date')
                .transform(lambda x: x.fillna(x.mean()))
                .fillna(0.0))
dbg(R_hat_filled.memory_usage(deep=True).sum() / 1e6)



# ─────────────────────────────────────────────────────────────────────────────
# Collect full weekly ElasticNet coefficients
coef_df = pd.DataFrame(columns=R_hat_filled.columns, dtype=float)
# ─────────────────────────────────────────────────────────────────────────────


if DEBUG:
    # average number of active (non-zero) forecasts per symbol-week
    nz_counts = (R_hat_filled.astype(bool)).sum(axis=1)
    avg_nz    = nz_counts.groupby(level='date').mean().mean()
    dbg(f"Avg active forecasts per row: {avg_nz:.1f} / {len(rank_cols)}")

# storage containers
theta_hist = {}                         # {Friday : pd.Series}
panel['CTREND'] = np.nan                # init

for end in dates[WIN:]:                 # loop over Fridays – starting after 1-yr burn-in
    start = end - pd.Timedelta(weeks=WIN)

    in_win = (R_hat_filled.index.get_level_values('date') >= start) & \
             (R_hat_filled.index.get_level_values('date') <  end)

    X_win = R_hat_filled.loc[in_win].astype('float32').values
    y_win = panel.loc[R_hat_filled.index[in_win], 'fwd_ret'].values

    ok = ~np.isnan(y_win)

    # require at least 20 observations per factor (paper’s rule)
    min_rows = 20 * len(rank_cols)
    if ok.sum() < min_rows:
        continue

    if DEBUG:
        # idx_train is the MultiIndex of the rows we’re actually fitting on
        idx_all   = R_hat_filled.index[in_win]   # all rows in the window
        idx_train = idx_all[ok]                 # then mask by ok
        n_rows    = len(idx_train)
        n_symbols = idx_train.get_level_values('symbol').nunique()
        dbg(f"[{end.date()}] training on {n_rows:,} rows ({n_symbols} symbols)")

        # ── DEBUG: report design matrix size ─────────────────────────────────
    if DEBUG:
        X_design = X_win[ok]
        dbg(f"[{end.date()}] Stage-2 design: {X_design.shape[0]:,} rows × {X_design.shape[1]:,} factors")

    enet2.fit(X_win[ok], y_win[ok])

    # record the full coefficient vector for this Friday
    coef_df.loc[end] = enet2.coef_

        # ── DEBUG: model performance & θ stats ───────────────────────────────
    if DEBUG:
        train_r2 = enet2.score(X_design, y_win[ok])
        dbg(f"[{end.date()}] Train R² = {train_r2:.4f}")
        theta    = pd.Series(enet2.coef_, index=R_hat_filled.columns)
        dbg(f"[{end.date()}] θ count={ (theta>0).sum() }, "
            f"mean={theta[theta>0].mean():.4f}, std={theta[theta>0].std():.4f}")

    # θ̂  – keep only positive weights as in eq. (11) :contentReference[oaicite:1]{index=1}
    theta = pd.Series(enet2.coef_, index=R_hat_filled.columns)
    theta = theta[theta > 0]

    theta_hist[end] = theta                # record for inspection / back-test

    # ----------------  build this Friday’s CTREND  ----------------
    idx_today = panel.loc[pd.IndexSlice[:, end], :].index      # rows for this Friday

    if not theta.empty:
        panel.loc[idx_today, 'CTREND'] = (
            R_hat_filled.loc[idx_today, theta.index]           # align rows+cols
                        .mean(axis=1)
        )
    else:
        panel.loc[idx_today, 'CTREND'] = np.nan

    if DEBUG:
        dbg(f"[{end.date()}] α*={enet2.alpha_:8.4g}  kept θ={len(theta):2d}")

# final tidy-up
panel['CTREND'] = panel['CTREND'].fillna(0.0)      # neutral where no signal
# recompute 'ctrend' Series from the panel we just filled
ctrend = panel['CTREND'].dropna()

if DEBUG:
    wstats = ctrend.groupby(level='date').mean().describe()
    dbg("\nCTREND weekly cross-sectional mean stats:")
    dbg(wstats.to_string())

# θ sparsity over time
pd.DataFrame({d: s.size for d,s in theta_hist.items()}, index=['#θ>0']).T.plot()

# CTREND coverage
coverage = panel['CTREND'].groupby(level='date').apply(lambda x: x.notna().mean())
coverage.plot(title="Fraction of symbols with CTREND each week")
# ─────────────────────────────────────────────────────────────────────────────
# 4.  QUICK LONG–SHORT SANITY CHECK  (equal-weight quintiles)
#      – you can replace with your full back-test block later
# ─────────────────────────────────────────────────────────────────────────────
def q5_q1_week(df):
    q = pd.qcut(df['CTREND'], 5, labels=False, duplicates='drop')
    long  = df[q == 4]['fwd_ret'].mean()
    short = df[q == 0]['fwd_ret'].mean()
    return long - short

ls_week = (panel.dropna(subset=['CTREND', 'fwd_ret'])
                 .groupby(level='date')
                 .apply(q5_q1_week))

if DEBUG:
    dbg("\nLong-short first 5 weeks (%):")
    dbg((ls_week.head()*100).round(2).to_string())
    sharpe = ls_week.mean() / ls_week.std(ddof=0) * np.sqrt(52)
    dbg(f"\nEqual-weight QS Sharpe ≈ {sharpe:.2f}")

et = panel['CTREND']
print(et.describe())
et.hist(bins=50)

import plotly.express as px

def plot_indicator_importance(
    coef_df,
    absolute: bool = True,
    top_n: int | None = None
) -> None:
    """
    Visualize normalized indicator importance (summing to 100%).

    Parameters
    ----------
    coef_df : pd.DataFrame
        Rows = dates, columns = factor names, values = ElasticNet weights.
    absolute : bool, default True
        If True, use mean(abs(coef)); else use signed mean(coef).
    top_n : int or None
        If set, only show the top_n indicators by normalized importance.
    """
    # 1) compute the “raw” importances
    if absolute:
        raw_imp = coef_df.abs().mean()
        title  = "Average Absolute Importance (normalized to 100%)"
    else:
        raw_imp = coef_df.mean().abs()
        title  = "Average Signed Importance (abs, normalized to 100%)"

    # 2) normalize so they sum to 100%
    norm_imp = raw_imp / raw_imp.sum() * 100

    # 3) sort (descending) and optionally trim to top_n
    norm_imp = norm_imp.sort_values(ascending=False)
    if top_n is not None:
        norm_imp = norm_imp.head(top_n)

    # 4) plot
    fig = px.bar(
        x=norm_imp.index,
        y=norm_imp.values,
        labels={"x": "Indicator", "y": "Importance (%)"},
        title=title
    )
    fig.update_layout(
        xaxis_tickangle=-45,
        margin=dict(l=40, r=40, t=60, b=120)
    )
    fig.show()

plot_indicator_importance(coef_df, absolute=True, top_n=60)

import numpy as np
import pandas as pd
from pandas.api.types import CategoricalDtype

# ─────────────────────────────────────────────────────────────────────────────
# 0.  CONFIG
# ─────────────────────────────────────────────────────────────────────────────
# Add a mode flag: True to clip returns at ± limits, False for raw returns
CLIP_RETURNS = True
UPPER_CLIP = 0.10
LOWER_CLIP = -0.10
cost_bp = 0
perf    = []
print("DT         | total |  non-null  |   top95    |   bot05    | long | short | status")

# ─────────────────────────────────────────────────────────────────────────────
# 1.  LOOP OVER WEEKS
# ─────────────────────────────────────────────────────────────────────────────
for dt, grp in panel.groupby(level='date'):
    total  = len(grp)
    # drop any symbols missing CTREND or fwd_ret
    grp    = grp.dropna(subset=['CTREND','fwd_ret'])
    # only trade symbols that are in the index
    grp    = grp[grp['in_index'] == 1]
    idx_ok = len(grp)

    if grp.empty:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} |     —     |     —     |    0 |     0 | SKIP")
        continue

    top95     = grp['CTREND'].quantile(0.95)
    bot05     = grp['CTREND'].quantile(0.05)
    long_grp  = grp[grp['CTREND'] > top95].copy()
    short_grp = grp[grp['CTREND'] < bot05].copy()

    n_long  = len(long_grp)
    n_short = len(short_grp)
    if n_long == 0 or n_short == 0:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | SKIP")
        continue

    #── compute realized vol over the *prior* 4 weeks ──────────────────────
    vol_long = (
        long_grp['fwd_ret']
             .shift(1)
             .rolling(4, min_periods=4)
             .std()
             .ffill()
    )
    vol_short = (
        short_grp['fwd_ret']
               .shift(1)
               .rolling(4, min_periods=4)
               .std()
               .ffill()
    )

    vol_long  = vol_long.clip(lower=1e-3)
    vol_short = vol_short.clip(lower=1e-3)


    # vol_long = 1
    # vol_short = 1

    # ── build weights by scaling CTREND by volatility ─────────────────────
    w_long  = long_grp['CTREND']  / vol_long
    w_short = -short_grp['CTREND'] / vol_short

    # collapse the MultiIndex so that w_long.index == symbols only
    w_long.index  = w_long.index.get_level_values('symbol')
    w_short.index = w_short.index.get_level_values('symbol')

    # ── record per‐symbol allocations & entry info ─────────────────────
    # fraction of leg notional each symbol gets
    norm_w_long = w_long / w_long.sum()
    alloc_long_pct  = {
        sym: pct for sym, pct in norm_w_long.items()
        if not np.isnan(pct)
    }

    norm_w_short = w_short / w_short.sum()
    alloc_short_pct = {
        sym: pct for sym, pct in norm_w_short.items()
        if not np.isnan(pct)
    }

    # entry prices (Friday close) and individual returns with clipping
    # entry_long_price  = {sym: weekly_data[sym].loc[dt]
    #                      for sym in long_grp.index.get_level_values('symbol')}
    # entry_short_price = {sym: weekly_data[sym].loc[dt]
    #                      for sym in short_grp.index.get_level_values('symbol')}

    entry_long_price = (
    long_grp['Close']
      .reset_index(level='date', drop=True)
      .to_dict()
    )
    entry_short_price = (
        short_grp['Close']
        .reset_index(level='date', drop=True)
        .to_dict()
    )

    # ── SKIP if either leg has zero total weight ─────────────────────────
    if w_long.sum() == 0 or w_short.sum() == 0:
        print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | ---- zero weight leg, SKIP")
        continue

    # ── compute weighted average returns ──────────────────────────────────
    # clip returns to avoid outliers
    # lr = long_grp['fwd_ret'].clip(lower=-0.10)
    # sr = short_grp['fwd_ret'].clip(upper=+0.10)
    if CLIP_RETURNS:
        lr = long_grp['fwd_ret'].clip(lower=LOWER_CLIP)
        sr = short_grp['fwd_ret'].clip(upper=UPPER_CLIP)
    else:
        lr = long_grp['fwd_ret']
        sr = short_grp['fwd_ret']

    # drop the 'date' level from the multi‐index → keys become just symbols
    ret_long_indiv  = lr.reset_index(level='date', drop=True).to_dict()
    ret_short_indiv = sr.reset_index(level='date', drop=True).to_dict()


    # compute exit price by applying the (clipped) return to the entry price
    exit_long_price = {
        sym: entry_long_price[sym] * (1 + ret_long_indiv[sym])
        for sym in entry_long_price
    }
    exit_short_price = {
        sym: entry_short_price[sym] * (1 + ret_short_indiv[sym])
        for sym in entry_short_price
    }

    long_ret  = (lr * w_long).sum()  / w_long.sum()
    short_ret = (sr * w_short).sum() / w_short.sum()
    spread    = long_ret - short_ret - 2*(cost_bp/1e4)

    # ── record symbol lists for post-mortem ──────────────────────────────
    long_syms  = long_grp.index.get_level_values('symbol').tolist()
    short_syms = short_grp.index.get_level_values('symbol').tolist()

    perf.append({
        'date':       dt,
        'total':      total,
        'non_null':   idx_ok,
        'top95':      top95,
        'bot05':      bot05,
        'n_long':     n_long,
        'n_short':    n_short,
        'long_ret':   long_ret,
        'short_ret':  short_ret,
        'spread':     spread,
        'long_syms':         long_syms,
        'short_syms':        short_syms,
        # new per‐trade details
        'long_alloc_pct':    alloc_long_pct,
        'long_entry_price':  entry_long_price,
        'long_ret_indiv':     ret_long_indiv,
        'long_exit_price':   exit_long_price,
        'short_alloc_pct':   alloc_short_pct,
        'short_entry_price': entry_short_price,
        'short_ret_indiv':    ret_short_indiv,
        'short_exit_price':  exit_short_price
    })

    print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | APPEND")

# ─────────────────────────────────────────────────────────────────────────────
# 2.  BUILD SUMMARY DATAFRAME & METRICS
# ─────────────────────────────────────────────────────────────────────────────
perf_df = pd.DataFrame(perf).set_index('date')

print("\nRESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
print("TOTAL WEEKS:", len(perf_df))
print(f"Avg weekly Q5–Q1 : {perf_df['spread'].mean():.2%}")
print(f"Ann. Sharpe       : {perf_df['spread'].mean()/perf_df['spread'].std(ddof=0)*np.sqrt(52):.2f}")
print(perf_df.tail())

# 1) Find the last date in your panel
last_date = panel.index.get_level_values('date')[-100]

# 2) Slice out that week’s data (symbols × all columns)
last_week = panel.xs(last_date, level='date')

# 3) Select just CTREND and fwd_ret
result = last_week[['CTREND', 'fwd_ret']]

# 4) (Optional) Sort by CTREND to see your top/bottom ideas
result_sorted = result.sort_values('CTREND', ascending=False)

print(f"Signals for {last_date.date()}:")
print(result_sorted.head(10))   # top 10
print(result_sorted.tail(10))   # bottom 10

# If you want the full list, just display `result`:
# display(result)

# look at the distribution of raw R_hat values
import numpy as np
vals = R_hat.values.flatten()
np.percentile(vals[~np.isnan(vals)], [1,5,25,50,75,95,99])

last_week

folder_path_residual_code = os.path.join(collab_base_path, "RESIDUAL CODE")
save_df_data_storage(panel, f"{folder_path_residual_code}/panel_v2_base.pkl")

# import numpy as np
# import pandas as pd
# from pandas.api.types import CategoricalDtype

# # ─────────────────────────────────────────────────────────────────────────────
# # 0.  CONFIG
# # ─────────────────────────────────────────────────────────────────────────────
# cost_bp = 0
# perf    = []
# print("DT         | total |  non-null  |   top95    |   bot05    | long | short | status")

# # ─────────────────────────────────────────────────────────────────────────────
# # 1.  LOOP OVER WEEKS
# # ─────────────────────────────────────────────────────────────────────────────
# for dt, grp in panel.groupby(level='date'):
#     total  = len(grp)
#     # drop any symbols missing CTREND or fwd_ret
#     grp    = grp.dropna(subset=['CTREND','fwd_ret'])
#     idx_ok = len(grp)

#     if grp.empty:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:9d} |     —     |     —     |    0 |     0 | SKIP")
#         continue

#     top95     = grp['CTREND'].quantile(0.95)
#     bot05     = grp['CTREND'].quantile(0.05)
#     long_grp  = grp[grp['CTREND'] > top95].copy()
#     short_grp = grp[grp['CTREND'] < bot05].copy()

#     n_long  = len(long_grp)
#     n_short = len(short_grp)
#     if n_long == 0 or n_short == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | SKIP")
#         continue

#     #── compute realized vol over the *prior* 4 weeks ──────────────────────
#     vol_long = (
#         long_grp['fwd_ret']
#              .shift(1)
#              .rolling(4, min_periods=4)
#              .std()
#              .ffill()
#     )
#     vol_short = (
#         short_grp['fwd_ret']
#                .shift(1)
#                .rolling(4, min_periods=4)
#                .std()
#                .ffill()
#     )

#     vol_long  = vol_long.clip(lower=1e-3)
#     vol_short = vol_short.clip(lower=1e-3)


#     # vol_long = 1
#     # vol_short = 1

#     # ── build weights by scaling CTREND by volatility ─────────────────────
#     w_long  = long_grp['CTREND']  / vol_long
#     w_short = -short_grp['CTREND'] / vol_short

#         # ── SKIP if either leg has zero total weight ─────────────────────────
#     if w_long.sum() == 0 or w_short.sum() == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | ---- zero weight leg, SKIP")
#         continue

#     # ── compute weighted average returns ──────────────────────────────────
#     # clip returns to avoid outliers
#     lr = long_grp['fwd_ret'].clip(lower=-0.10)
#     sr = short_grp['fwd_ret'].clip(upper=+0.10)

#     long_ret  = (lr * w_long).sum()  / w_long.sum()
#     short_ret = (sr * w_short).sum() / w_short.sum()
#     spread    = long_ret - short_ret - 2*(cost_bp/1e4)

#     # ── record symbol lists for post-mortem ──────────────────────────────
#     long_syms  = long_grp.index.get_level_values('symbol').tolist()
#     short_syms = short_grp.index.get_level_values('symbol').tolist()

#     perf.append({
#         'date':       dt,
#         'total':      total,
#         'non_null':   idx_ok,
#         'top95':      top95,
#         'bot05':      bot05,
#         'n_long':     n_long,
#         'n_short':    n_short,
#         'long_ret':   long_ret,
#         'short_ret':  short_ret,
#         'spread':     spread,
#         'long_syms':  long_syms,
#         'short_syms': short_syms
#     })

#     print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top95:9.4f} | {bot05:9.4f} | {n_long:4d} | {n_short:4d} | APPEND")

# # ─────────────────────────────────────────────────────────────────────────────
# # 2.  BUILD SUMMARY DATAFRAME & METRICS
# # ─────────────────────────────────────────────────────────────────────────────
# perf_df = pd.DataFrame(perf).set_index('date')

# print("\nRESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
# print("TOTAL WEEKS:", len(perf_df))
# print(f"Avg weekly Q5–Q1 : {perf_df['spread'].mean():.2%}")
# print(f"Ann. Sharpe       : {perf_df['spread'].mean()/perf_df['spread'].std(ddof=0)*np.sqrt(52):.2f}")
# print(perf_df.tail())

if False:
    df_ctrend = panel['CTREND'].unstack(level='symbol') * 100
    df_ctrend = df_ctrend.reset_index()
    # Build DataFrame and export
    folder_store_analysis_results = os.path.join(collab_base_path, "RESULTS  STRATEGY", "ANALYSIS OVER 1X")
    trades_executed_excel_name = "CTREND SCORES.xlsx"
    xls_path = os.path.join(folder_store_analysis_results, trades_executed_excel_name)
    df_ctrend.to_excel(xls_path, index=False)
    print(f"Trade log written to {xls_path}")

df_ctrend

weekly_data = {
  sym: df['Close'].resample('W-FRI').last()
  for sym, df in df_data_storage.items()
}



import numpy as np

# assume perf_df is your DataFrame with a 'spread' column
factors     = perf_df['spread'] + 1
total_rtn   = factors.prod()
n_weeks     = len(factors)

cagr = total_rtn ** (52/n_weeks) - 1
print(f"Total return   : {total_rtn - 1:.2%}")
print(f"CAGR           : {cagr:.2%}")

perf_df.loc['2025-04-25']['long_ret_indiv']

import pandas as pd

def export_trades_details(perf_df: pd.DataFrame, output_path: str = "Trades_Details.xlsx"):
    """
    Exports a detailed trade log from perf_df to an Excel file.
    Each row corresponds to a single symbol trade, with entry/exit info and returns.
    """
    records = []
    dates = list(perf_df.index)

    for i, dt in enumerate(dates[:-1]):
        exit_dt = dates[i + 1]
        row = perf_df.loc[dt]
        spread = row['spread']

        # Unpack longs
        for sym, alloc in row['long_alloc_pct'].items():
            entry_price = row['long_entry_price'][sym]
            exit_price  = row['long_exit_price'][sym]
            ret_indiv   = row['long_ret_indiv'][sym]
            records.append({
                'week_start': dt,
                'exit_date': exit_dt,
                'symbol': sym,
                'side': 'long',
                'alloc_pct': alloc,
                'entry_price': entry_price,
                'exit_price': exit_price,
                'return_indiv': ret_indiv,
                'spread': spread
            })

        # Unpack shorts
        for sym, alloc in row['short_alloc_pct'].items():
            entry_price = row['short_entry_price'][sym]
            exit_price  = row['short_exit_price'][sym]
            ret_indiv   = row['short_ret_indiv'][sym]
            records.append({
                'week_start': dt,
                'exit_date': exit_dt,
                'symbol': sym,
                'side': 'short',
                'alloc_pct': alloc,
                'entry_price': entry_price,
                'exit_price': exit_price,
                'return_indiv': ret_indiv,
                'spread': spread
            })

    # Build DataFrame and export
    trades_df = pd.DataFrame.from_records(records)
    folder_store_analysis_results = os.path.join(collab_base_path, "RESULTS  STRATEGY", "ANALYSIS OVER 1X")
    trades_executed_excel_name = "AI_LONG_SHORT.xlsx"
    xls_path = os.path.join(folder_store_analysis_results, trades_executed_excel_name)
    trades_df.to_excel(xls_path, index=False)
    print(f"Trade log written to {xls_path}")

# Example usage:
export_trades_details(perf_df)

"""BACKTESTING STRATEGY **HERE**"""

import pandas as pd
import numpy as np
from pathlib import Path
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ========== PARAMETERS ==========
initial_capital = 1_000_000  # ₹1M
cost_bp = 0                  # zero transaction cost

# ========== PREPARE WEEKLY CLOSES ==========
weekly_data = {
    sym: df['Close'].resample('W-FRI').last()
    for sym, df in df_data_storage.items()
}

# ========== RUN 2× LEVERAGED BACKTEST (FULL DEPLOYMENT) ==========
dates = sorted(panel.index.get_level_values('date').unique())
equity = initial_capital
equity_series = []
equity_dates = []
trade_log = []

for i in range(len(dates) - 1):
    dt = dates[i]
    next_dt = dates[i + 1]
    grp = panel.xs(dt, level='date').dropna(subset=['CTREND', 'fwd_ret'])

    if grp.empty:
        equity_series.append(equity)
        equity_dates.append(next_dt)
        continue

    # Signal cutoffs
    top95 = grp['CTREND'].quantile(0.95)
    bot05 = grp['CTREND'].quantile(0.05)
    long_grp = grp[grp['CTREND'] > top95].copy()
    short_grp = grp[grp['CTREND'] < bot05].copy()

    if long_grp.empty or short_grp.empty:
        equity_series.append(equity)
        equity_dates.append(next_dt)
        continue

    # Realized vol over prior 4 weeks
    vol_long = (long_grp['fwd_ret'].shift(1)
                        .rolling(4, min_periods=4).std().ffill().clip(lower=1e-3))
    vol_short = (short_grp['fwd_ret'].shift(1)
                          .rolling(4, min_periods=4).std().ffill().clip(lower=1e-3))

    # Raw weights
    w_long = long_grp['CTREND'] / vol_long
    w_short = -short_grp['CTREND'] / vol_short  # positive values for shorts

    sum_w_long = w_long.sum()
    sum_w_short = w_short.sum()
    if sum_w_long == 0 or sum_w_short == 0:
        equity_series.append(equity)
        equity_dates.append(next_dt)
        continue

    # **Match CODE ALPHA leverage**: deploy 100% of equity in each leg
    long_notional_total = equity
    short_notional_total = equity

    # -- Process long trades (fractional shares) --
    for sym, weight in w_long.items():
        if dt not in weekly_data[sym].index or next_dt not in weekly_data[sym].index:
            continue
        entry_price = weekly_data[sym].loc[dt]
        exit_price_raw = weekly_data[sym].loc[next_dt]
        if pd.isna(entry_price) or pd.isna(exit_price_raw):
            continue

        raw_ret = exit_price_raw / entry_price - 1
        clipped_ret = np.clip(raw_ret, -0.10, 0.10)
        exit_price = entry_price * (1 + clipped_ret)

        pos_notional = long_notional_total * (weight / sum_w_long)
        shares = pos_notional / entry_price  # fractional
        if not np.isfinite(shares):
            continue

        pnl = shares * (exit_price - entry_price)
        equity += pnl

        trade_log.append({
            'symbol': sym,
            'side': 'long',
            'entry_date': dt,
            'exit_date': next_dt,
            'entry_price': entry_price,
            'exit_price': exit_price,
            'shares': shares,
            'pnl': pnl
        })

    # -- Process short trades (fractional shares) --
    for sym, weight in w_short.items():
        if dt not in weekly_data[sym].index or next_dt not in weekly_data[sym].index:
            continue
        entry_price = weekly_data[sym].loc[dt]
        exit_price_raw = weekly_data[sym].loc[next_dt]
        if pd.isna(entry_price) or pd.isna(exit_price_raw):
            continue

        raw_ret = exit_price_raw / entry_price - 1
        clipped_ret = np.clip(raw_ret, -0.10, 0.10)
        exit_price = entry_price * (1 + clipped_ret)

        pos_notional = short_notional_total * (weight / sum_w_short)
        shares = pos_notional / entry_price  # fractional
        if not np.isfinite(shares):
            continue

        pnl = shares * (entry_price - exit_price)
        equity += pnl

        trade_log.append({
            'symbol': sym,
            'side': 'short',
            'entry_date': dt,
            'exit_date': next_dt,
            'entry_price': entry_price,
            'exit_price': exit_price,
            'shares': shares,
            'pnl': pnl
        })

    # Record equity at week’s close
    equity_series.append(equity)
    equity_dates.append(next_dt)

# Build equity curve & drawdown
equity_curve = pd.Series(equity_series, index=equity_dates)
drawdown = equity_curve / equity_curve.cummax() - 1

# ========== PLOT EQUITY & DRAWDOWN ==========
fig = make_subplots(
    rows=2, cols=1, shared_xaxes=True,
    vertical_spacing=0.12,
    subplot_titles=('Equity Curve (2× leverage)', 'Drawdown')
)
fig.add_trace(
    go.Scatter(x=equity_curve.index, y=equity_curve.values, name='Equity'),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(x=drawdown.index, y=drawdown.values, name='Drawdown'),
    row=2, col=1
)
fig.update_layout(
    height=700, width=900,
    title_text='Long–Short Backtest with 2× Leverage',
    showlegend=False
)
fig.show()

# ========== EXPORT TRADES ==========
trades_df = pd.DataFrame(trade_log)
# output_path = Path('/mnt/data/Trades.xlsx')
# trades_df.to_excel(output_path, index=False)
# print(f"Trades.xlsx saved to: {output_path}")

# 1) From the summary DataFrame:
avg_weekly_from_perf = perf_df['spread'].mean()
print(f"Average weekly return (from perf_df): {avg_weekly_from_perf:.2%}")

# 2) Or from your equity curve:
weekly_rets = equity_curve.pct_change().dropna()
avg_weekly_from_equity = weekly_rets.mean()
print(f"Average weekly return (from equity_curve): {avg_weekly_from_equity:.2%}")

eq_on_active = equity_curve.loc[perf_df.index]
avg_active = eq_on_active.pct_change().mean()
print(f"Avg weekly return on active weeks: {avg_active:.2%}")

# compute weekly returns
weekly_rets = equity_curve.pct_change().dropna()

# for each formation-Friday dt, find the next Friday in the equity index
active_rets = []
for dt in perf_df.index:
    next_friday = weekly_rets.index[weekly_rets.index > dt][0]
    active_rets.append( weekly_rets.loc[next_friday] )

print(f"Corrected avg weekly return on active weeks: {np.mean(active_rets):.2%}")

trades_df

perf_df.loc["2025-04-25"]["long_syms"]

# after your existing importances calculation…
coef_df = pd.DataFrame(coef_hist)
importances = coef_df.abs().mean().sort_values(ascending=False)

# convert to percentage of total importance
importances_pct = 100 * importances / importances.sum()

# view the top 20 in percent terms
print(importances_pct.head(20).round(2))

# plot
top20 = importances_pct.head(20)
plt.figure(figsize=(8,4))
plt.barh(top20.index, top20.values)
plt.gca().invert_yaxis()
plt.xlabel("Mean |Coefficient| (% of total)")
plt.title("Top 20 Signal Importances (in %)")
plt.show()

# # ─────────────────────────────────────────────────────────────────────────────
# # 3.  WEEKLY LONG–SHORT BACK-TEST  (index-members only)
# # ─────────────────────────────────────────────────────────────────────────────
# qtype   = CategoricalDtype(['Q1','Q2','Q3','Q4','Q5'], ordered=True)
# cost_bp = 0
# perf = []

# for dt, grp in panel_weekly.groupby(level='date'):
#     grp = grp[(grp['in_index'] == 1) & (~grp['ETREND'].isna())].copy()
#     if grp.empty: continue

#     '''
#     # inside your dt loop, after filtering grp:
#     uniq = grp['ETREND'].nunique()
#     n_bins = min(5, uniq)                             # at most 5, but fewer if ties
#     labels = [f"Q{i+1}" for i in range(n_bins)]       # e.g. ['Q1','Q2','Q3']
#     grp['bucket'] = pd.qcut(
#         grp['ETREND'],
#         q=n_bins,
#         labels=labels,
#         duplicates="drop"
#     )
#     if grp['bucket'].isna().any():
#         continue
#     '''

#     '''
#     long_grp  = grp[grp['bucket'] == 'Q5']
#     '''
#         # ── define fixed 20% thresholds
#     top20    = grp['ETREND'].quantile(0.80)
#     bot20    = grp['ETREND'].quantile(0.20)

#     long_grp  = grp[grp['ETREND'] >= top20]
#     short_grp = grp[grp['ETREND'] <= bot20]

#     # if either side is empty (rare), skip:
#     if long_grp.empty or short_grp.empty:
#         continue
#     # ── Stop‐loss floor for longs (no worse than –10 %)
#     long_grp['ret_impl']  = long_grp['fwd_ret'].clip(lower=-0.10)
#     vol_long  = long_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_long    = long_grp['ETREND'] / vol_long            # ETREND ÷ vol  :contentReference[oaicite:1]{index=1}
#     if w_long.sum() == 0:
#         continue
#     long      = (long_grp['fwd_ret'] * w_long).sum() / w_long.sum()
#     # w_long    = long_grp['ETREND']                                 # weights = forecast
#     # if w_long.sum() == 0:
#     #     continue
#     # long      = (long_grp['fwd_ret'] * w_long).sum() / w_long.sum()

#     '''
#     short_grp = grp[grp['bucket'] == 'Q1']
#     '''
#     # ── Stop‐loss cap for shorts (no worse than +10 % loss)
#     short_grp['ret_impl'] = short_grp['fwd_ret'].clip(upper=+0.10)
#     vol_short = short_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_short   = -short_grp['ETREND'] / vol_short
#     if w_short.sum() == 0:
#         continue
#     short     = (short_grp['fwd_ret'] * w_short).sum() / w_short.sum()
#     # w_short   = -short_grp['ETREND']                               # invert sign for shorts
#     # if w_short.sum() == 0:
#     #     continue
#     # short     = (short_grp['fwd_ret'] * w_short).sum() / w_short.sum()
#     spread = long - short - 2 * cost_bp / 1e4
#     perf.append((dt, long, short, spread))

# perf = pd.DataFrame(perf, columns=['date','Long','Short','Spread']).set_index('date')

# ann_factor = np.sqrt(52)
# print(f"Avg weekly Q5–Q1 : {perf['Spread'].mean():.3%}")
# print(f"Ann. Sharpe       : {perf['Spread'].mean()/perf['Spread'].std()*ann_factor:.2f}")

# # ─────────────────────────────────────────────────────────────────────────────
# # 3.  WEEKLY LONG–SHORT BACK-TEST  (index-members only) WITH DEBUG LOGS
# # ─────────────────────────────────────────────────────────────────────────────
# qtype   = CategoricalDtype(['Q1','Q2','Q3','Q4','Q5'], ordered=True)
# cost_bp = 0
# perf    = []

# print("DT        | #total | #in_index |  top20    | bot20    | #long | #short | appended?")

# for dt, grp in panel_weekly.groupby(level='date'):
#     total = len(grp)
#     grp   = grp[(grp['in_index'] == 1) & (~grp['ETREND'].isna())]
#     idx_ok = len(grp)
#     if grp.empty:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:9d} |   SKIPPED (no index or ETREND)")
#         continue

#     # compute thresholds
#     top20 = grp['ETREND'].quantile(0.80)
#     bot20 = grp['ETREND'].quantile(0.20)

#     long_grp  = grp[grp['ETREND'] >= top20]
#     short_grp = grp[grp['ETREND'] <= bot20]

#     n_long  = len(long_grp)
#     n_short = len(short_grp)

#     # skip if empty
#     if n_long == 0 or n_short == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top20:8.4f} | {bot20:8.4f} | "
#               f"{n_long:6d} | {n_short:6d} | SKIPPED (empty leg)")
#         continue

#     # ... your stop-loss + weight logic here ...

#     # if we get this far, we append
#     perf.append((dt, long, short, spread))
#     print(f"{dt.date()} | {total:6d} | {idx_ok:9d} | {top20:8.4f} | {bot20:8.4f} | "
#           f"{n_long:6d} | {n_short:6d} | APPENDED")

# # after loop:
# perf_df = pd.DataFrame(perf, columns=['date','Long','Short','Spread']).set_index('date')
# print()
# print("RESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
# print("TOTAL WEEKS:", len(perf_df))

# import numpy as np
# import pandas as pd
# from pandas.api.types import CategoricalDtype

# qtype   = CategoricalDtype(['Q1','Q2','Q3','Q4','Q5'], ordered=True)
# cost_bp = 0
# perf    = []

# # DEBUG HEADER
# print("DT         | total | in_index |   top20   |   bot20   | long  | short | status")

# for dt, grp in panel_weekly.groupby(level='date'):
#     total    = len(grp)
#     grp      = grp[(grp['in_index'] == 1) & (~grp['ETREND'].isna())]
#     idx_ok   = len(grp)

#     if grp.empty:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} |     —     |     —     |    0  |     0 | SKIP (no data)")
#         continue

#     # fixed quintiles
#     top20     = grp['ETREND'].quantile(0.90)
#     bot20     = grp['ETREND'].quantile(0.10)
#     long_grp  = grp[grp['ETREND'] >= top20]
#     short_grp = grp[grp['ETREND'] <= bot20]

#     n_long   = len(long_grp)
#     n_short  = len(short_grp)
#     if n_long == 0 or n_short == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | SKIP (empty leg)")
#         continue

#     # long leg
#     long_grp['ret_impl'] = long_grp['fwd_ret'].clip(lower=-0.10)
#     vol_long  = long_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_long    = long_grp['ETREND'] / vol_long
#     #w_long    = long_grp['ETREND']
#     if w_long.sum() == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | SKIP (zero w_long)")
#         continue
#     long_ret = (long_grp['fwd_ret'] * w_long).sum() / w_long.sum()

#     # short leg
#     short_grp['ret_impl'] = short_grp['fwd_ret'].clip(upper=+0.10)
#     vol_short = short_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_short   = -short_grp['ETREND'] / vol_short
#     #w_short   = -short_grp['ETREND']
#     if w_short.sum() == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | SKIP (zero w_short)")
#         continue
#     short_ret = (short_grp['fwd_ret'] * w_short).sum() / w_short.sum()

#     spread = long_ret - short_ret - 2 * cost_bp / 1e4
#     perf.append((dt, long_ret, short_ret, spread))
#     print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | APPEND")

# # Build the DataFrame and summary
# perf_df = pd.DataFrame(perf, columns=['date','Long','Short','Spread']).set_index('date')

# print("\nRESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
# print("TOTAL WEEKS:", len(perf_df))
# print(f"Avg weekly Q5–Q1 : {perf_df['Spread'].mean():.3%}")
# print(f"Ann. Sharpe       : {perf_df['Spread'].mean()/perf_df['Spread'].std()*np.sqrt(52):.2f}")

# import numpy as np
# import pandas as pd
# from pandas.api.types import CategoricalDtype

# qtype   = CategoricalDtype(['Q1','Q2','Q3','Q4','Q5'], ordered=True)
# cost_bp = 0
# perf    = []

# print("DT         | total | in_index |   top20   |   bot20   | long  | short | status")

# for dt, grp in panel_weekly.groupby(level='date'):
#     total  = len(grp)
#     grp    = grp[(grp['in_index'] == 1) & (~grp['ETREND'].isna())]
#     idx_ok = len(grp)

#     if grp.empty:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} |     —     |     —     |    0  |     0 | SKIP (no data)")
#         continue

#     top20     = grp['ETREND'].quantile(0.95)
#     bot20     = grp['ETREND'].quantile(0.05)
#     long_grp  = grp[grp['ETREND'] >= top20]
#     short_grp = grp[grp['ETREND'] <= bot20]

#     n_long   = len(long_grp)
#     n_short  = len(short_grp)
#     if n_long == 0 or n_short == 0:
#         print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | SKIP (empty leg)")
#         continue

#     # weights + returns
#     long_grp['ret_impl']   = long_grp['fwd_ret'].clip(lower=-0.10)
#     #vol_long  = long_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     vol_long = (
#     long_grp['fwd_ret']
#         .shift(1)                               # drop the still-unknown week
#         .rolling(4, min_periods=4)              # need four realised weeks
#         .std()
#         .ffill()                                # optional: carry last known σ forward
#     )
#     w_long    = long_grp['ETREND'] / vol_long
#     long_ret  = (long_grp['fwd_ret'] * w_long).sum() / w_long.sum()

#     short_grp['ret_impl'] = short_grp['fwd_ret'].clip(upper=+0.10)
#     # vol_short = short_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     vol_short = (
#     short_grp['fwd_ret']           # raw forward returns
#         .shift(1)                  # throw away the still-unknown coming week
#         .rolling(4, min_periods=4) # need four fully realised weeks
#         .std()                     # compute σ
#         .ffill()                   # (optional) carry last known σ forward
#     )
#     w_short   = -short_grp['ETREND'] / vol_short
#     short_ret = (short_grp['fwd_ret'] * w_short).sum() / w_short.sum()

#     spread = long_ret - short_ret - 2 * cost_bp / 1e4

#     # extract the symbol lists
#     long_syms  = long_grp.index.get_level_values('symbol').tolist()
#     short_syms = short_grp.index.get_level_values('symbol').tolist()

#     # append *all* details as a dict
#     perf.append({
#         'date':        dt,
#         'total':       total,
#         'in_index':    idx_ok,
#         'top20':       top20,
#         'bot20':       bot20,
#         'n_long':      n_long,
#         'n_short':     n_short,
#         'long_ret':    long_ret,
#         'short_ret':   short_ret,
#         'spread':      spread,
#         'long_syms':   long_syms,
#         'short_syms':  short_syms
#     })

#     print(f"{dt.date()} | {total:6d} | {idx_ok:8d} | {top20:8.4f} | {bot20:8.4f} | {n_long:5d} | {n_short:5d} | APPEND")

# # Build the DataFrame
# perf_df = pd.DataFrame(perf).set_index('date')

# print("\nRESULT ▶ first/last date:", perf_df.index.min(), "/", perf_df.index.max())
# print("TOTAL WEEKS:", len(perf_df))
# print(f"Avg weekly Q5–Q1 : {perf_df['spread'].mean():.3%}")
# print(f"Ann. Sharpe       : {perf_df['spread'].mean()/perf_df['spread'].std()*np.sqrt(52):.2f}")
# # Now you have columns:
# # ['total','in_index','top20','bot20','n_long','n_short',
# #  'long_ret','short_ret','spread','long_syms','short_syms']
# print(perf_df.tail())

import plotly.graph_objects as go

# 1) Prepare your time series (as above)
initial_capital = 1_000_000
equity    = (1 + perf_df['spread']).cumprod() * initial_capital
running_max = equity.cummax()
drawdown    = (equity - running_max) / running_max

# 2) Make the equity‐curve plot
fig_eq = go.Figure()
fig_eq.add_trace(go.Scatter(
    x=equity.index, y=equity.values,
    mode='lines', name='Equity Curve'
))
fig_eq.update_layout(
    title='Interactive Equity Curve',
    xaxis_title='Date',
    yaxis_title='Portfolio Value (₹)'
)

# 3) Make the drawdown plot
fig_dd = go.Figure()
fig_dd.add_trace(go.Scatter(
    x=drawdown.index, y=drawdown.values,
    mode='lines', name='Drawdown',
    line=dict(color='firebrick')
))
fig_dd.update_layout(
    title='Interactive Drawdown Profile',
    xaxis_title='Date',
    yaxis_title='Drawdown (%)',
    yaxis_tickformat='.1%'
)

# 4) Show them
fig_eq.show()
fig_dd.show()

print(perf_df.tail(40))

import numpy as np

# 70/30 split of your backtest
split_idx  = int(len(perf_df) * 0.7)
perf_in    = perf_df.iloc[:split_idx]
perf_out   = perf_df.iloc[split_idx:]

def sharpe(r):
    return r.mean() / r.std() * np.sqrt(52)

print("In-sample Sharpe :", sharpe(perf_in['spread']))
print("Out-of-sample Sharpe:", sharpe(perf_out['spread']))

import matplotlib.pyplot as plt

obs_sharpe = sharpe(perf_df['spread'])
n_sims      = 1000
shuffled    = []

for _ in range(n_sims):
    rnd = perf_df['spread'].sample(frac=1, replace=False).reset_index(drop=True)
    shuffled.append(sharpe(rnd))

p_value = np.mean([s >= obs_sharpe for s in shuffled])

# plot null-distribution
plt.hist(shuffled, bins=50, edgecolor='k')
plt.axvline(obs_sharpe, color='r', linewidth=2, label=f"Observed ({obs_sharpe:.2f})")
plt.title(f"Permutation Sharpe Distribution (p≈{p_value:.3f})")
plt.legend()
plt.show()

print(f"Permutation p-value: {p_value:.3f}")

import matplotlib.pyplot as plt

def sharpe(r): return r.mean() / r.std() * np.sqrt(52)

window = 104  # weeks
rolling = perf_df['spread'] \
    .rolling(window) \
    .apply(lambda x: sharpe(x), raw=False) \
    .dropna()

rolling.plot(title="2-Year Rolling Sharpe", figsize=(10,4))
plt.axhline(0, linestyle='--', color='gray')
plt.show()

# qtype   = CategoricalDtype(['Q1','Q2','Q3','Q4','Q5'], ordered=True)
# cost_bp = 0
# perf = []

# for dt, grp in panel_weekly.groupby(level='date'):
#     grp = grp[(grp['in_index'] == 1) & (~grp['ETREND'].isna())].copy()
#     if grp.empty: continue

#     # inside your dt loop, after filtering grp:
#     uniq = grp['ETREND'].nunique()
#     n_bins = min(5, uniq)                             # at most 5, but fewer if ties
#     labels = [f"Q{i+1}" for i in range(n_bins)]       # e.g. ['Q1','Q2','Q3']
#     grp['bucket'] = pd.qcut(
#         grp['ETREND'],
#         q=n_bins,
#         labels=labels,
#         duplicates="drop"
#     )
#     if grp['bucket'].isna().any():
#         continue

#     # long  = grp.loc[grp['bucket'] == 'Q5', 'fwd_ret'].mean()
#     # short = grp.loc[grp['bucket'] == 'Q1', 'fwd_ret'].mean()
#     long_grp  = grp[grp['bucket'] == 'Q5']
#     # ── Stop‐loss floor for longs (no worse than –10 %)
#     long_grp['ret_impl']  = long_grp['fwd_ret'].clip(lower=-0.10)

#     vol_long  = long_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_long    = long_grp['ETREND'] / vol_long            # ETREND ÷ vol  :contentReference[oaicite:1]{index=1}
#     if w_long.sum() == 0:
#         continue
#     long      = (long_grp['fwd_ret'] * w_long).sum() / w_long.sum()

#     short_grp = grp[grp['bucket'] == 'Q1']
#     # ── Stop‐loss cap for shorts (no worse than +10 % loss)
#     short_grp['ret_impl'] = short_grp['fwd_ret'].clip(upper=+0.10)
#     vol_short = short_grp['fwd_ret'].rolling(4).std().fillna(method='bfill')
#     w_short   = -short_grp['ETREND'] / vol_short
#     if w_short.sum() == 0:
#         continue
#     short     = (short_grp['fwd_ret'] * w_short).sum() / w_short.sum()

#     spread = long - short - 2 * cost_bp / 1e4
#     perf.append((dt, long, short, spread))

# perf = pd.DataFrame(perf, columns=['date','Long','Short','Spread']).set_index('date')

# ann_factor = np.sqrt(52)
# print(f"Avg weekly Q5–Q1 : {perf['Spread'].mean():.3%}")
# print(f"Ann. Sharpe       : {perf['Spread'].mean()/perf['Spread'].std()*ann_factor:.2f}")

